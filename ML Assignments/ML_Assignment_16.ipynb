{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06900de6",
   "metadata": {},
   "source": [
    "#### 1. In a linear equation, what is the difference between a dependent variable and an independent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3c000",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Dependent variable is the one for which we want to make predictions. There is only one dependent variable.\n",
    "\n",
    "Independent variables are the ones using which we make predictions. There could be more than one independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e4be",
   "metadata": {},
   "source": [
    "#### 2. What is the concept of simple linear regression? Give a specific example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efbcce",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Simple Linear Regression is a type of Regression algorithms that models the relationship between a dependent variable and a single independent variable. The relationship shown by a Simple Linear Regression model is linear or a sloped straight line, hence it is called Simple Linear Regression. The key point in Simple Linear Regression is that the dependent variable must be a continuous/real value. However, the independent variable can be measured on continuous or categorical values.\n",
    "\n",
    "Example: To predict the sales of a product based on the amount spent on adverstisement, we can use simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461791e",
   "metadata": {},
   "source": [
    "#### 3. In a linear regression, define the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de89b4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Slope is the amount by which the dependent variable changes when there is one unit change in the independent variable while keeping rest of the factors constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772bdfc",
   "metadata": {},
   "source": [
    "#### 4. Determine the graph's slope, where the lower point on the line is represented as (3, 2) and the higher point is represented as (2, 2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097bfaa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "We have, (x1,y1)=(3,2) and (x2,y2)=(2,2)\n",
    "\n",
    "Slope = (y2-y1)/(x2-x1) = (2-2)/(3-2) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e351",
   "metadata": {},
   "source": [
    "#### 5. In linear regression, what are the conditions for a positive slope?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca371",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The condition for positive slope is that as the independent variable x increases, the dependent variable y must increase on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf127dd",
   "metadata": {},
   "source": [
    "#### 6. In linear regression, what are the conditions for a negative slope?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3da423",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The condition for negative slope is that as the independent variable x increases, the dependent variable y must decrease on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dee030",
   "metadata": {},
   "source": [
    "#### 7. What is multiple linear regression and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0d00f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Multiple Linear Regression is a type of Regression algorithms that models the relationship between a dependent variables and one or more independent variables. The relationship shown by a Multiple Linear Regression model is linear or a sloped hyperplane, hence it is called Multiple Linear Regression. The key point in Multiple Linear Regression is that the dependent variable must be a continuous/real value. However, the independent variables can be measured on continuous or categorical values.\n",
    "\n",
    "Example: To predict the sales of a product based on the amount spent on different advertisement media like TV, radio and newspaper, we can use simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543f58",
   "metadata": {},
   "source": [
    "#### 8. In multiple linear regression, define the sum of squares due to error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6cf06",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The sum of squares due to error is the sum of all the squared differences between the true value and predicted value.The sum of squares due to error essentially measures the variation of modeling errors. In other words, it depicts how much of the variation in the dependent variable in a regression model cannot be explained by the model. Generally, a lower residual sum of squares indicates that the regression model can better explain the data, while a higher residual sum of squares indicates that the model poorly explains the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6353249",
   "metadata": {},
   "source": [
    "#### 9. In multiple linear regression, define the sum of squares due to regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159a04e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The sum of squares due to regression is the difference between total sum of squares and sum of squares due to error. It measures the variation in the dependent variable in a regression model that is explained by the model. Generally, a higher sum of squares due to regression indicates that the regression model has performed well in explaining the variation in the data, while a lower sum of squares due to regression indicates that the model poorly explains the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca4b00",
   "metadata": {},
   "source": [
    "#### 10. In a regression equation, what is multicollinearity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03422d4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Multicollinearity is said to exist in a regression equation when two or more independent variables in the equation are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb680faa",
   "metadata": {},
   "source": [
    "#### 11. What is heteroskedasticity, and what does it mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba7cd",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Hetero means multiple and skedasticity means variance. When the errors do not have a constant variance the we say that the regression model has heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a7089",
   "metadata": {},
   "source": [
    "#### 12. Describe the concept of ridge regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbe7a8",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. In ridge regression, the loss function contains an extra penalty term which is the L2 norm of the vector of the parameters multiplied by lambda. Hence ridge regression is also called L2 regularization. It helps in overcoming overfitting problem. As we increase the value of lambda, the value of the estimated parameter reduces but it never reaches zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0767f70",
   "metadata": {},
   "source": [
    "#### 13. Describe the concept of lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bf11d",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The word “LASSO” stands for Least Absolute Shrinkage and Selection Operator. Lasso regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L1 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. In lasso regression, the loss function contains an extra penalty term which is the L1 norm of the vector of the parameters multiplied by lambda. Hence lasso regression is also called L1 regularization. It helps in overcoming overfitting problem. As we increase the value of lambda, the value of the estimated parameter reduces and some parameters become zero. Hence, lasso also helps in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091f315",
   "metadata": {},
   "source": [
    "#### 14. What is polynomial regression and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c60fc",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Polynomial Regression is a regression algorithm that models the relationship between a dependent(y) and independent variable(x) as nth degree polynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). It is a linear model with some modification in order to increase the accuracy. The dataset used in Polynomial regression for training is of non-linear nature. It makes use of a linear regression model to fit the complicated and non-linear functions and datasets. Hence, in Polynomial regression, the original features are converted into Polynomial features of required degree (2,3,..,n) and then modeled using a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62134a36",
   "metadata": {},
   "source": [
    "#### 15. Describe the basis function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45b7b0",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In a regression model, we can replace the variables by some functions of the variables. These functions are called the basis functions. This is a generalization of linear regression that essentially replaces each input with a function of the input. (A linear basis function model that uses the identity function is just linear regression.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be018654",
   "metadata": {},
   "source": [
    "#### 16. Describe how logistic regression works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7bdfa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Logistic regression is a supervised machine learning algorith which is used for the purpose of classification. It first uses the linear regression function and thean applies the sigmoid function over it so that the final values ranges between 0 and 1. These values tend to estimate the probabilities of the data points lying in a particular class. We assign the data point to that class for which the probability is highest. In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38afd4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
