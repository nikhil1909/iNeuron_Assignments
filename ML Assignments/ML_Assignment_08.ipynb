{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b19f78",
   "metadata": {},
   "source": [
    "#### 1. What exactly is a feature? Give an example to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f0fe2",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "A feature is a variable that takes different values which are the different datapoints for that specific feature. For example, height of people is one feature that may have different values for different person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4550ce70",
   "metadata": {},
   "source": [
    "#### 2. What are the various circumstances in which feature construction is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0658f23",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Feature construcion is required when the given data is not able to give good quality model. In that case we need to use use datat to construct suitable features that give good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91028cc4",
   "metadata": {},
   "source": [
    "#### 3. Describe how nominal variables are encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad753bfe",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Nominal variables are discrete values with no ordering among them. For example, colors, vehicles, animal species, etc. If we want to encode three colors red, green and blue then we can use 1 for indicating the color that is present and 0 for the rest of the colors(that are not present) in our data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f5af2",
   "metadata": {},
   "source": [
    "#### 4. Describe how numeric features are converted to categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1874a",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "We cn use binning to convert numeric features to categorical. There are three binning methods equal width binning, equal frequency binning and k-means binning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77e38c3",
   "metadata": {},
   "source": [
    "#### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c790a",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In wrapper methods, the feature selection process is based on a specific machine learning algorithm that we are trying to fit on a given dataset.\n",
    "\n",
    "It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The evaluation criterion is simply the performance measure which depends on the type of problem, for e.g. For regression evaluation criterion can be p-values, R-squared, Adjusted R-squared, similarly for classification the evaluation criterion can be accuracy, precision, recall, f1-score, etc. Finally, it selects the combination of features that gives the optimal results for the specified machine learning algorithm.\n",
    "\n",
    "Advantages:\n",
    "1. Takes less time to make decision.\n",
    "2. Consists of 3 different methods. Forwad, Bakward and Mixed.\n",
    "\n",
    "Disdvantages:\n",
    "1. The model it gives might not be the best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ef88e4",
   "metadata": {},
   "source": [
    "#### 6. When is a feature considered irrelevant? What can be said to quantify it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ba076",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "A feature is considered irrelevant when it does not play a vital role in making predictions. We can use various feature selection methods which make use of various metrics to decide which features are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa83160",
   "metadata": {},
   "source": [
    "#### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038204c",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "If two features {X1, X2} are highly correlated, then then one of them can be considered redundant feature since it have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77100c9",
   "metadata": {},
   "source": [
    "#### 8. What are the various distance measurements used to determine feature similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2f847",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Various distance measurements used to determine feature similarity are Euclidean distance, Manhattan distance, Minkowski distance, Cosine similarity and Jaccard similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5654cd",
   "metadata": {},
   "source": [
    "#### 9. State difference between Euclidean and Manhattan distances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707fdfca",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. In a simple way of saying it is the total sum of the difference between the x-coordinates and y-coordinates. In a plane with P at coordinate (x1, y1) and Q at (x2, y2). Manhattan distance between P and Q = |x1 – x2| + |y1 – y2|.\n",
    "\n",
    "The Euclidean distance between two points in either the plane or 3-dimensional space measures the length of a segment connecting the two points. It is the most obvious way of representing distance between two points. The Pythagorean Theorem can be used to calculate the distance between two points. Mathematically it computes the root of squared differences between the coordinates between two objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db603a6",
   "metadata": {},
   "source": [
    "#### 10. Distinguish between feature transformation and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b759781",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In feature selection, we use various techniques like Filtering, Wrapper search and embedded technique to select the features that are relevant.\n",
    "\n",
    "In feature transformation, we transform the features by imputing the missing values, handling categorical variables, outlier detection and feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbea679",
   "metadata": {},
   "source": [
    "#### 11. Make brief notes on any two of the following:\n",
    "\n",
    "1. SVD (Standard Variable Diameter)\n",
    "\n",
    "2. Collection of features using a hybrid approach\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86054ac",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. SVD: The Singular Value Decomposition (SVD) of a matrix is a factorization of that matrix into three matrices. It has some interesting algebraic properties and conveys important geometrical and theoretical insights about linear transformations. \n",
    "\n",
    "2. Collection of features using a hybrid approach: In feature selection, we use various techniques like Filtering, Wrapper search and embedded technique to select the features that are relevant. An approach which uses a mix of all the three given techniques is called hybrid approach for feature selection.\n",
    "\n",
    "3. The width of the silhouette: The width of silhouette is used to get the optimal value of number of clusters to be made in a clustering algorithm. The value of silhouettee coefficient for a point liest between -1 and 1. -1 indicates cluster is bad and 1 indicates cluster is good.\n",
    "\n",
    "4. Receiver operating characteristic curve: Curve plotted between True Positive Rate and False Positive Rate is Receiver Operating Characteristics curve and is used to find the area under the curve for ROC-AUC score for binary classification evaluation. True Positive Rate and False Positive Rate are calculated for different thresholds values where thresholds take values starting from the highest probability scores assigned to data points and goes up to the lowest probability score. The curve is impacted by presence of outliers, and simple models. Extensions can be made to this curve to suit multiclass classification evaluation requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1044baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
