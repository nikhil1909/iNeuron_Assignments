{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06900de6",
   "metadata": {},
   "source": [
    "#### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3c000",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Key reasons:\n",
    "1. Helps in speeding up the training process.\n",
    "2. Less computational cost\n",
    "3. Removes multicollinearity.\n",
    "4. Removes overfitting.\n",
    "5. Helps in visualization.\n",
    "6. Less complex model.\n",
    "7. Few algorithms like KNN do not perform well on high dimenasional dataset.\n",
    "\n",
    "Disadvantages:\n",
    "1. Data or information loss\n",
    "2. Transformed features are often hard to interpret.\n",
    "3. Computationally intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e4be",
   "metadata": {},
   "source": [
    "#### 2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efbcce",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Curse of Dimensionality refers to a set of problems that arise when working with high-dimensional data. The dimension of a dataset corresponds to the number of attributes/features that exist in a dataset. A dataset with a large number of attributes, generally of the order of a hundred or more, is referred to as high dimensional data. Some of the difficulties that come with high dimensional data manifest during analyzing or visualizing the data to identify patterns, and some manifest while training machine learning models. The difficulties related to training machine learning models due to high dimensional data are referred to as the ‘Curse of Dimensionality’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461791e",
   "metadata": {},
   "source": [
    "#### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de89b4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "No, dimensionality reduction is not completely reversible. Some amount of data is lost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772bdfc",
   "metadata": {},
   "source": [
    "#### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097bfaa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Yes, PCA can be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e351",
   "metadata": {},
   "source": [
    "#### 5. Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca371",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Depending upon the dataset, any number of dimensions between 1 to 950 could be used to exaplain 95% variance ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf127dd",
   "metadata": {},
   "source": [
    "#### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3da423",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Vanilla PCA: PCA is an unsupervised dimensionality reduction technique which is widely used in machine learning. It reduces the dimension of the data by projecting them onto a lower-dimensional subspace. However, while reducing the dimension of the data, we need to preserve as much information about the original data as possible. PCA preserves most of the variance of the original data by transforming to a new set of variables called principal components, which are linear combinations of the variables in the original data. These principal components are uncorrelated and are ordered in such a way that the first few principal components retain most of the variance of the original data.\n",
    "\n",
    "Kernel PCA: PCA is a linear method. It works great for linearly separable datasets. However, if the dataset has non-linear relationships, then it produces undesirable results.Kernel PCA is a technique which uses the so-called kernel trick and projects the linearly inseparable data into a higher dimension where it is linearly separable.There are various kernels that are popularly used; some of them are linear, polynomial, RBF, and sigmoid.\n",
    "\n",
    "Randomized PCA: The classical PCA uses the low-rank matrix approximation to estimate the principal components. However, this method becomes costly and makes the whole process difficult to scale, for large datasets. By randomizing how the singular value decomposition of the dataset happens, we can approximate the first K principal components quickly than classical PCA.\n",
    "\n",
    "Incremental PCA: The above-discussed methods require the whole training dataset to fit in the memory. Incremental PCA can be used when the dataset is too large to fit in the memory. Here we split the dataset into mini-batches where each batch can fit into the memory and then feed it one mini-batch at a moment to the IPCA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dee030",
   "metadata": {},
   "source": [
    "#### 7. How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0d00f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "A dimensionality reduction algorithm is said to work well if it eliminates a significant number of dimensions from the dataset without losing too much information. Moreover, the use of dimensionality reduction in preprocessing before training the model allows measuring the performance of the second algorithm.\n",
    "We can therefore infer if an algorithm performed well if the dimensionality reduction does not lose too much information after applying the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543f58",
   "metadata": {},
   "source": [
    "#### 8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6cf06",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Yes, we can use two different dimensionality reduction algorithms in a chain. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae66d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
