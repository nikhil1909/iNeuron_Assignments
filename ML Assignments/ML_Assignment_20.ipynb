{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06900de6",
   "metadata": {},
   "source": [
    "#### 1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3c000",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e4be",
   "metadata": {},
   "source": [
    "#### 2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efbcce",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Support vectors are those vectors that help in deciding to choose the hyperplane. On changing these support vectors, the hyperplane changes. So, in a way, these vectors support the hyperplane and thats why they are called support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461791e",
   "metadata": {},
   "source": [
    "#### 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de89b4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "If the scale of one of the variable is very large than the rest of the variables then that variable dominates in making decisions. This creates biased results. To get rid of this problem, w need to scale the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772bdfc",
   "metadata": {},
   "source": [
    "#### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097bfaa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Yes, there are methods using which we can get confidence score and percentage chance after classifying a case using SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e351",
   "metadata": {},
   "source": [
    "#### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca371",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In the dual form, the optimization problem contains only the inner product of training data points  xTi⋅xj , which is a convenient form in which to introduce the the notion of feature mapping used in non-linearly separable cases by simply replacing the inner product term with  ϕ(xi)T⋅ϕ(xj) , where  ϕ(.)  is the feature mapping function. In the primal form, it’s not so obvious how to incorporate the feature mapping function into the optimization algorithm. So, we should train model using dual form of SVM problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf127dd",
   "metadata": {},
   "source": [
    "#### 6. Let&#39;s say you&#39;ve used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3da423",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Gamma decides that how much curvature we want in a decision boundary. Gamma parameter is used only when we are working with RBF kernel. Gamma high means more curvature. Gamma low means less curvature. So to handle undefitting, we need to raise the gamma. \n",
    "\n",
    "Lowering the C value would decrease the error in the training data so it would be better to lower the C to stop the model from underfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dee030",
   "metadata": {},
   "source": [
    "#### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0d00f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The QP parametrs can be chosen by using the hyperparameter tuning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543f58",
   "metadata": {},
   "source": [
    "#### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6cf06",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Since all the models use the support vector machine algorithm only, so by tuing the parameters we can create similar models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d6f2d",
   "metadata": {},
   "source": [
    "#### 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a56e9",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "We can achieve a precision level as high as 95% or more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8706f5",
   "metadata": {},
   "source": [
    "#### 10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e666a24",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "We need to first import the dataset, then we can do some preprocessing on it. After that we can use SVMRegressor module of sklearn library to train the model. Also we can use gridsearchcv module to tune the hyperparameters to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31be2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
