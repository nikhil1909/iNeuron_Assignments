{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3c42c7",
   "metadata": {},
   "source": [
    "#### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700304dc",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Feature engineering is the process of using domain knowledge to select, manipulate, and transform raw data into features to improve the performance of machine learning models. Various expects of feature engineering are Feature Transformation, Feature Construction(manual), Feature Selection and Feature Extraction(programming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569acc54",
   "metadata": {},
   "source": [
    "#### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935985fc",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Feature selection is the process of selecting features that are relevant for training our model and removing rest of the features from the dataset. The aim of it is to reduce the computational cost for model training. Various methods of feature selection are Filtering, Wrapper and Embedded technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23fc7c",
   "metadata": {},
   "source": [
    "#### 3. Describe the feature selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fdf85b",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Filtering: The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. It uses correlation, lda, anova or chi-square. \n",
    "\n",
    "Pros: \n",
    "1. It is faster and usually the better approach when the number of features are huge.\n",
    "2. Rely entirely on features in the data set\n",
    "3. Avoids overfitting.\n",
    "\n",
    "Cons:\n",
    "1. The filter method looks at individual features for identifying it’s relative importance. A feature may not be useful on its own but maybe an important influencer when combined with other features. Filter methods may miss such features.\n",
    "2. Does not remove multicollinearity.\n",
    "3. May fail to select beast features.\n",
    "\n",
    "Wrapper approach: In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "Pros:\n",
    "1. Gives good performance.\n",
    "2. Computatinally less expensive than embedded method.\n",
    "3. Better generalization.\n",
    "4. Robust interaction \n",
    "\n",
    "Cons:\n",
    "1. Computationally expensive than filtering.\n",
    "2. Prone to overfitting.\n",
    "3. May fail to select beast features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a74a5",
   "metadata": {},
   "source": [
    "#### 4. Answer the following:\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used feature extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2dad78",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "i. Feature selection is the process of selecting features that are relevant for training our model and removing rest of the features from the dataset. The aim of it is to reduce the computational cost for model training. Various methods of feature selection are Filtering, Wrapper and Embedded technique.\n",
    "\n",
    "ii. Feature extraction uses algorithms that adds penalty term to the loss function to regulate the parameters which helps in avoiding overfitting problem and feature selection. Most widely used feature selection algorithms are L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9c4e8",
   "metadata": {},
   "source": [
    "#### 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c15e2f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In text data, feature engineering is much more cumbersome as the data requires lots of preprocessing like removing irrelevant symbols, removing extra spaces and so on. After preprocessing of text data, we can use various methods like bags of words, bag of n-gram, tf-idf method for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c18e753",
   "metadata": {},
   "source": [
    "#### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f75b32",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ba0787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two given rows is 0.6753032524419089\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = [2, 3, 2, 0, 2, 3, 3, 0, 1]\n",
    "y = [2, 1, 0, 0, 3, 2, 1, 3, 1]\n",
    "dot = np.dot(x,y)\n",
    "x_rss = np.sqrt(sum(np.square(x)))\n",
    "y_rss = np.sqrt(sum(np.square(y)))\n",
    "cos_similarity = dot/(x_rss*y_rss)\n",
    "print(\"Cosine similarity between the two given rows is\",cos_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62222850",
   "metadata": {},
   "source": [
    "#### 7. Answer the following:\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c437e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Hamming distance = sum-over-i(|xi-yi|) where xi and yi is the value of vector x and y respectively at index i.\n",
    "Hamming distance between 10001011 and 11001111 is 2 because the numbers are not matching at 2 indices.\n",
    "\n",
    "M00 = 2, \n",
    "M11 = 4,\n",
    "M01 = 1,\n",
    "M10 = 1\n",
    "\n",
    "Similarity matching coefficient = (M00+M11)/(M00+M11+M01+M10) = 6/8 = 0.75\n",
    "\n",
    "Jaccard index = M11/(M01+M10+M11) = 4/8 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56022a1e",
   "metadata": {},
   "source": [
    "#### 8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f8887",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "High-dimensional data set means data set that has very large number of features. Letf1, f2, ... fn denoten different features then if the valur of n is 100 or 1000 then these set of features will be called high dimensional data set. In ML these data set result in very high computational cost. To get rid of this we may use feature selection or feature extraction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2e9eb",
   "metadata": {},
   "source": [
    "#### 9. Make a few quick notes on:\n",
    "\n",
    "1. PCA(Principle Component Analysis)\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169d128",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. PCA(Principle Component Analysis): Principal component analysis is a technique for feature extraction — so it combines our input variables in a specific way, then we can drop the “least important” variables while still retaining the most valuable parts of all of the variables! As an added benefit, each of the “new” variables after PCA are all independent of one another. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another. If we decide to fit a linear regression model with these “new” variables (see “principal component regression” below), this assumption will necessarily be satisfied.\n",
    "\n",
    "2. Use of vectors: Vectors can be used to easily convert and represent categorical and text data into numerical forms. Futher these vectors are easy for futher computational tasks.\n",
    "\n",
    "3. Embedded technique: Embedded techniques are used for overfitting and feature selection process. These techniques use methods like L1 and L2 regularization in which we use a penalty term to regulate the parameters to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6c003",
   "metadata": {},
   "source": [
    "#### 10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4435b915",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. Sequential backward exclusion vs. sequential forward selection: In forward slection, we use some metric say p-value and keep on adding the variables one by one as long as the p-value of the model is below the threshold p-value. In backward selection, we do the similar thing but here we use the full model and keep on removing the variables one by one which result into the largest p-value.\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper: In filtering, the selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here. It uses correlation, lda, anova or chi-square. In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "3. SMC vs. Jaccard coefficient: Both are used for measuring the similarity between two vectors. M11 is the number of indices at which both vectors have value 1. M00 is the number of indices at which both vectors have value 0. M01 is the number of indices at which first vector has value 0 and second vector has value 1. M10 is the number of indices at which first vector has value 1 and second vector has value 0. Then,\n",
    "\n",
    "SMC = (M00+M11)/(M00+M11+M01+M10) and Jaccard index = M11/(M01+M10+M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf1561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
