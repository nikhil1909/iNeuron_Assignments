{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06900de6",
   "metadata": {},
   "source": [
    "#### 1. Recognize the differences between supervised, semi-supervised, and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3c000",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The differences between supervised, semi-supervised, and unsupervised learning are:\n",
    "\n",
    "Supervised learning aims to learn a function that, given a sample of data and desired outputs, approximates a function that maps inputs to outputs.\n",
    "\n",
    "Semi-supervised learning aims to label unlabeled data points using knowledge learned from a small number of labeled data points.\n",
    "\n",
    "Unsupervised learning does not have (or need) any labeled outputs, so its goal is to infer the natural structure present within a set of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e4be",
   "metadata": {},
   "source": [
    "#### 2. Describe in detail any five examples of classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efbcce",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. Spam filtering: An algorithm is trained to recognize spam email by learning the characteristics of what constitutes spam vs non-spam email. The classification model could be a function that maps from an email text to a spam classification (or non-spam classification). \n",
    "2. Document classification: A multinomial classification model can be trained to classify documents in different categories. In this case, the classification model can be thought of as a function that maps from a document to a category label. \n",
    "3. Image classification: One of the most popular classification problems is image classification: determining what type of object (or scene) is in a digital image. Images can be thought of as a high-dimensional vectors which we would like to classify into different classes such as cat, car, human, and airplane. A multinomial classification model can be trained to classify images into different categories. \n",
    "4. Malware classification: A multinomial classification can be used to categorize new/emerging-malware based on similar malware traits. Malware classification is extremely helpful for security specialists in determining the best course of action for combatting and preventing malware.\n",
    "5. Customer behaviour prediction: Customers can be divided into groups based on their purchasing habits, online shop browsing habits, and other factors. Classification models, for example, can be used to evaluate whether or not a consumer is likely to purchase additional things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461791e",
   "metadata": {},
   "source": [
    "#### 3. Describe each phase of the classification process in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de89b4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The classification process can be divided into five phases:\n",
    "1. Data acquisition: This is the gathering of the data from some source or database.\n",
    "2. Data preprocessing: In this phase, we clean our data to make a good model.\n",
    "3. Feature extraction: We extract the features that appear to be more suitable for creating the classification model.\n",
    "4. Create model: In this phase, we use different classification algorithms to create models.\n",
    "5. Test model: This is the last phase in which we choose the best model after testing all the created models on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772bdfc",
   "metadata": {},
   "source": [
    "#### 4. Go through the SVM model in depth using various scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097bfaa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning. The goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane. SVM chooses the extreme points/vectors that help in creating the hyperplane. These extreme cases are called as support vectors, and hence algorithm is termed as Support Vector Machine. SVM can be understood with the example that we have used in the KNN classifier. Suppose we see a strange cat that also has some features of dogs, so if we want a model that can accurately identify whether it is a cat or dog, so such a model can be created by using the SVM algorithm. We will first train our model with lots of images of cats and dogs so that it can learn about different features of cats and dogs, and then we test it with this strange creature. So as support vector creates a decision boundary between these two data (cat and dog) and choose extreme cases (support vectors), it will see the extreme case of cat and dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e351",
   "metadata": {},
   "source": [
    "#### 5. What are some of the benefits and drawbacks of SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca371",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Benefits of SVM:\n",
    "1. Effective in high dimensional cases\n",
    "2. Its memory efficient as it uses a subset of training points in the decision function called support vectors\n",
    "3. Different kernel functions can be specified for the decision functions and its possible to specify custom kernels\n",
    "\n",
    "Drawbacks of SVM:\n",
    "1. It doesn’t perform well when we have large data set because the required training time is higher\n",
    "2. It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf127dd",
   "metadata": {},
   "source": [
    "#### 6. Go over the kNN model in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3da423",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique. It assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories. It stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm. It can be used for Regression as well as for Classification but mostly it is used for the Classification problems. It is a non-parametric algorithm, which means it does not make any assumption on underlying data. It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset. KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n",
    "\n",
    "Example: Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dee030",
   "metadata": {},
   "source": [
    "#### 7. Discuss the kNN algorithm&#39;s error rate and validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0d00f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Training error rate: This is the error rate that is computed based on training data. This error rate decreases as we decrease the values of K.\n",
    "\n",
    "Validation error rate: This is the error rate that is computed using the cross-validation validation set. This error rate helps in choosing an efficient value of K.\n",
    "\n",
    "Test error rate: This is the error rate that is computed using the test set. This error rate tells the final performance of the model based on the unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543f58",
   "metadata": {},
   "source": [
    "#### 8. For kNN, talk about how to measure the difference between the test and training results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6cf06",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The difference between the test and training results can be measured by plotting a graph where x-axis should be 1/K and y-axis should be the error rates for the corresponding values of K. On plotting this graph one might observe that for training set the errors reduce on average as e increase the value of 1/K. And for test set the errors initially reduce then start increasing. We choose that value of K  after which the error rates started increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6353249",
   "metadata": {},
   "source": [
    "#### 9. Create the kNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159a04e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The K-NN working can be explained on the basis of the below algorithm:\n",
    "1. Select the number K of the neighbors\n",
    "2. Calculate the Euclidean distance of K number of neighbors\n",
    "3. Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "4. Among these k neighbors, count the number of the data points in each category.\n",
    "5. Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "6. Our model is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca4b00",
   "metadata": {},
   "source": [
    "#### 10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03422d4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. It is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions. It is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure. In a Decision tree, there are three nodes, which are the Root Node, the Decision Node and Leaf Node. Root node is the strating point of the tree which has the least entropy. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb680faa",
   "metadata": {},
   "source": [
    "#### 11. Describe the different ways to scan a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba7cd",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Different ways to scan a decision tree are:\n",
    "1. Gini index: Gini Index is a score that evaluates how good a split is by how mixed the classes are in the split's two groups. Gini index could have a score between values 0 and 1, where 0 is when all observations belong to one class, and 1 is a random distribution of the elements within classes. In this case, we want to have a Gini index score as low as possible.\n",
    "2. Entropy and Information Gain: As a starter, Entropy is defined as the measurement of impurity or uncertainty within a dataset. This means that entropy measures the number of class observations mixing within the data.  Information Gain is a difference of entropy before and after the split by a feature. In other words, Information Gain measure the impurity reduces after splitting; it means what we want is the highest Information Gain score because the highest information gain means that the splitting resulted in a more homogenous result.\n",
    "3. Variance reduction: Variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable Y due to the split at this node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6a7089",
   "metadata": {},
   "source": [
    "#### 12. Describe in depth the decision tree algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbe7a8",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. It is a graphical representation for getting all the possible solutions to a problem/decision based on given conditions. It is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure. In a Decision tree, there are three nodes, which are the Root Node, the Decision Node and Leaf Node. Root node is the strating point of the tree which has the least entropy. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0767f70",
   "metadata": {},
   "source": [
    "#### 13. In a decision tree, what is inductive bias? What would you do to stop overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544bf11d",
   "metadata": {},
   "source": [
    "#### Ans: \n",
    "Before learning a model given a data and a learning algorithm, there are a few assumptions a learner makes about the algorithm. These assumptions are called the inductive bias. It is like the property of the algorithm. For eg. in the case of decision trees, the depth of the tress is the inductive bias. If the depth of the tree is too low, then there is too much generalisation in the model. Similarly, if the depth of the tree is too much, there is too less generalisation and while testing the model on a new example. Shorter trees are preferred over longer ones. Trees that place high information gain attributes close to the root are preferred over those that do not.\n",
    "\n",
    "To stop overfitting, one should prefer a smaller tree over a large tree. One can also use random forest model to stop overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7091f315",
   "metadata": {},
   "source": [
    "#### 14.Explain advantages and disadvantages of using a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65c60fc",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Advantages:\n",
    "1. It is simple to understand as it follows the same process which a human follow while making any decision in real-life.\n",
    "2. It can be very useful for solving decision-related problems.\n",
    "3. It helps to think about all the possible outcomes for a problem.\n",
    "4. There is less requirement of data cleaning compared to other algorithms.\n",
    "\n",
    "Disadvantages:\n",
    "1. The decision tree contains lots of layers, which makes it complex.\n",
    "2. It may have an overfitting issue, which can be resolved using the Random Forest algorithm.\n",
    "3. For more class labels, the computational complexity of the decision tree may increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62134a36",
   "metadata": {},
   "source": [
    "####  15. Describe in depth the problems that are suitable for decision tree learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45b7b0",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. Instances are represented by attribute-value pairs.\n",
    "2. The target function has discrete output values.\n",
    "3. Disjunctive descriptions may be required.\n",
    "4. The training data may contain errors.\n",
    "5. The training data may contain missing attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be018654",
   "metadata": {},
   "source": [
    "#### 16. Describe in depth the random forest model. What distinguishes a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd71ad1",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression. One of the most important features of the Random Forest Algorithm is that it can handle the data set containing continuous variables as in the case of regression and categorical variables as in the case of classification. It performs better results for classification problems. The greater number of trees in the forest leads to higher accuracy and prevents the problem of overfitting. \n",
    "\n",
    "Unlike most of the other supervised machine learning algorithm, random forest is an ensemble learning method,  which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0f796",
   "metadata": {},
   "source": [
    "#### 17. In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1698f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "OOB error is the out-of-bag error which is the proportion of those misclassified data points of every tree that were not used in making that tree.\n",
    "\n",
    "Variables with high importance are drivers of the outcome and their values have a significant impact on the outcome values. By contrast, variables with low importance might be omitted from a model, making it simpler and faster to fit and predict. There are two measures of importance given for each variable in the random forest. The first measure is based on how much the accuracy decreases when the variable is excluded. This is further broken down by outcome class. The second measure is based on the decrease of Gini impurity when a variable is chosen to split a node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48e111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
