{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408e4b62",
   "metadata": {},
   "source": [
    "#### 1. In the sense of machine learning, what is a model? What is the best way to train a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e9005",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "A machine learning model is defined as a mathematical representation of the output of the training process. It is a program that has been trained to find patterns within new data and make predictions.\n",
    "\n",
    "The best way to train a model is get the data, then clean it and perform data pre-processing, then the data is ready to build different models, then we can choose the model that gives the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123539c",
   "metadata": {},
   "source": [
    "#### 2. In the sense of machine learning, explain the &quot;No Free Lunch&quot; theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c53fa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "No free lunch theorem says that there is no universally best algorithm, i.e. there is no algortihm that perform better than the rest of the algorithms on all problems, given that there are no assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03505132",
   "metadata": {},
   "source": [
    "#### 3. Describe the K-fold cross-validation mechanism in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d22d5",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In k-fold cv, we divide our dataset D randomly into k mutually exclusive and exhaustive subsets. Let s1, s2,...., sk be those k subsets. Then we perform k iterations and in each iteration we use one subset as the validation set and the rest of the subsets as training set, and store the performance. Then we average out those performance metrics to get the final performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee9a09",
   "metadata": {},
   "source": [
    "#### 4. Describe the bootstrap sampling method. What is the aim of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655eee4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In bootstrap sampling method, we take samples from the data with replacement and then estimate the parameters using those samples and we do this multiple times and the final estimation of the parameter is the average of all the estimated parameters from different samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1ae32",
   "metadata": {},
   "source": [
    "#### 5. What is the significance of calculating the Kappa value for a classification model? Demonstrate how to measure the Kappa value of a classification model using a sample collection of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea07b7d",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Kappa value or Cohen's Kappa coefficient is an evaluation metric for classification models. Its significance as an evaluation metric is that it can be used to evaluate multi class classification models and also works on models trained on imbalanced datasets(scores like accuracy scores fail for imbalanced datasets). In simpler words, it basically tells you how much better your classifier is performing over the performance of a classifier that simply guesses at random according to the frequency of each class. Cohen's kappa is always less than or equal to 1. Values of 0 or less, indicate that the classifier is useless Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.\n",
    "\n",
    "Kappa value = (po-pe)/(1-pe) where po is the observed probability of agreement and pe is expected probability of agreement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d33cdd",
   "metadata": {},
   "source": [
    "#### 6. Describe the model ensemble method. In machine learning, what part does it play?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50fca7",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Model ensemble method is a technique in which we use more than one model to make the predictions. The models that are used must be different from each other. This difference is created either by using different algorithms or by using the same algorithm on different sets of data. Voting ensemble, bagging, boosting and stacking are the four types of ensemble methods. \n",
    "\n",
    "In machine learning, ensemble method plays an import part of in making more precise decision that has low bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5940b",
   "metadata": {},
   "source": [
    "#### 7. What is a descriptive model&#39;s main purpose? Give examples of real-world problems that descriptive models were used to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e1d50",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "A descriptive model's main purpose is to get insights or inferences from the data that are otherwise not easy to get just by looking a the data. Market segmentation is the problem that descriptive model is used to solve. Unsupervised machine learning algorithms are used to create descriptive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66888c5b",
   "metadata": {},
   "source": [
    "#### 8. Describe how to evaluate a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da50cb5",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "We can use R-square, Adjusted R-square, mean absolute error or rrot meean squared error to evaluate regreession model. These are the metrics that best summarize the performance of a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d208ac7c",
   "metadata": {},
   "source": [
    "#### 9. Distinguish :\n",
    "\n",
    "1. Descriptive vs. predictive models\n",
    "\n",
    "2. Underfitting vs. overfitting the model\n",
    "\n",
    "3. Bootstrapping vs. cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3da84e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. Descriptive vs. predictive models: Descriptive models are used to get insights or inferences from the data while predictive models are used to make predictions.\n",
    "\n",
    "2. Underfitting vs. overfitting the model: An undesfitting model is the one that perform poorly on both training and test data  while an overfitting model is the one that performs well on training dat but porly on test data.\n",
    "\n",
    "3. Bootstrapping vs. cross-validation: In bootstrapping, we create new samples by taking samples fro original dataset with replacement. In cross validation we divide the data in k subsets and in each iteration we use one subset as validation set and rest as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d36df",
   "metadata": {},
   "source": [
    "#### 10. Make quick notes on:\n",
    "\n",
    "1. LOOCV.\n",
    "\n",
    "2. F-measurement\n",
    "\n",
    "3. The width of the silhouette\n",
    "\n",
    "4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f78bb",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "1. LOOCV: In LOOCV, in each iteration only one data point is used as validation set and rest of the data points is used as training set.\n",
    "\n",
    "2. F-measurement: F-measurement is nothing but the harmonic mean of precision and recall which is used to indicate how well a classification model has performed.\n",
    "\n",
    "3. The width of the silhouette: The width of silhouette is used to get the optimal value of number of clusters to be made in a clustering algorithm. The value of silhouettee coefficient for a point liest between -1 and 1. -1 indicates cluster is bad and 1 indicates cluster is good.\n",
    "\n",
    "4. Receiver operating characteristic curve: Curve plotted between True Positive Rate and False Positive Rate is Receiver Operating Characteristics curve and is used to find the area under the curve for ROC-AUC score for binary classification evaluation. True Positive Rate and False Positive Rate are calculated for different thresholds values where thresholds take values starting from the highest probability scores assigned to data points and goes up to the lowest probability score. The curve is impacted by presence of outliers, and simple models. Extensions can be made to this curve to suit multiclass classification evaluation requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5310abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
