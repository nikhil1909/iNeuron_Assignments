{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06900de6",
   "metadata": {},
   "source": [
    "#### 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e3c000",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Supervised learning is a sub-category of machine learning in which we solve problems that is supervised by a variable. For example, in predicting the price of houses using variables like no. of rooms, bedrooms, location, etc, we can use the already known prices to evaluate the performance of the model that we have created.\n",
    "\n",
    "Unupervised learning is a sub-category of machine learning in which we solve problems that is not supervised by any variable. For example, in market segmentation problem, we divide the customers based on their charachteristics into different groups. After the group is created, we do not have any variable that we can use to supervise the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a5e4be",
   "metadata": {},
   "source": [
    "#### 2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61efbcce",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Products Segmentation, Customer Segmentation, Similarity Detection, Recommendation Systems and Labelling unlabelled datasets are few applications of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461791e",
   "metadata": {},
   "source": [
    "#### 3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de89b4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The various types of clustering are:\n",
    "\n",
    "Connectivity-based Clustering (Hierarchical clustering): Hierarchical Clustering is a method of unsupervised machine learning clustering where it begins with a pre-defined top to bottom hierarchy of clusters. It then proceeds to perform a decomposition of the data objects based on this hierarchy, hence obtaining the clusters. This method follows two approaches based on the direction of progress, i.e., whether it is the top-down or bottom-up flow of creating clusters.\n",
    "\n",
    "Centroids-based Clustering (Partitioning methods): Centroid based clustering is considered as one of the most simplest clustering algorithms, yet the most effective way of creating clusters and assigning data points to it. The intuition behind centroid based clustering is that a cluster is characterized and represented by a central vector and data points that are in close proximity to these vectors are assigned to the respective clusters.\n",
    "\n",
    "Density-based Clustering (Model-based methods): If one looks into the previous two methods that we discussed, one would observe that both hierarchical and centroid based algorithms are dependent on a distance (similarity/proximity) metric. The very definition of a cluster is based on this metric. Density-based clustering methods take density into consideration instead of distances. Clusters are considered as the densest region in a data space, which is separated by regions of lower object density and it is defined as a maximal-set of connected points.\n",
    "\n",
    "Distribution-based Clustering: Until now, the clustering techniques as we know are based around either proximity (similarity/distance) or composition (density). There is a family of clustering algorithms that take a totally different metric into consideration â€“ probability. Distribution-based clustering creates and groups data points based on their likely hood of belonging to the same probability distribution (Gaussian, Binomial etc.) in the data.\n",
    "\n",
    "Fuzzy Clustering: The general idea about clustering revolves around assigning data points to mutually exclusive clusters, meaning, a data point always resides uniquely inside a cluster and it cannot belong to more than one cluster. Fuzzy clustering methods change this paradigm by assigning a data-point to multiple clusters with a quantified degree of belongingness metric. The data-points that are in proximity to the center of a cluster, may also belong in the cluster that is at a higher degree than points in the edge of a cluster. The possibility of which an element belongs to a given cluster is measured by membership coefficient that vary from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772bdfc",
   "metadata": {},
   "source": [
    "#### 4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097bfaa",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Using the distance based metrics like Within cluster sum of squares, k-means clustering determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e351",
   "metadata": {},
   "source": [
    "#### 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ca371",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In k-means clustering, in each iteration we compute the centroid of the cluster and then assign the data points to the updated centroids. In k-medoids algorithm, we choose randomly k medoids and then assign points to these medoids, then we replace one of the mediod with another data point and again assign points to the updated set of medoids, then we compute the cost metric for the two set of clusters and we keep that set of clusters that has lesser cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf127dd",
   "metadata": {},
   "source": [
    "#### 6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3da423",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters. The key to interpreting a dendrogram is to focus on the height at which any two objects are joined together. In the example above, we can see that E and F are most similar, as the height of the link that joins them together is the smallest. The next two most similar objects are A and B. In the dendrogram above, the height of the dendrogram indicates the order in which the clusters were joined. A more informative dendrogram can be created where the heights reflect the distance between the clusters as is shown below. In this case, the dendrogram shows us that the big difference between clusters is between the cluster of A and B versus that of C, D, E, and F."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dee030",
   "metadata": {},
   "source": [
    "#### 7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0d00f",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "SSE is sum of squared errors. In k-means algorithm, we have to calculate SSE for a cluster which is the sum of squared difference of all the points from the centroid of that cluster. To get a good cluster, we are required to minimize this SSE value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1543f58",
   "metadata": {},
   "source": [
    "#### 8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6cf06",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "k-means is one of the simplest unsupervised learning algorithms that solve the well known clustering problem. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters) fixed apriori. The main idea is to define k centers, one for each cluster.\n",
    "\n",
    "Step 1: Choose the number of clusters k.\n",
    "Step 2: Select k random points from the data as centroids.\n",
    "Step 3: Assign all the points to the closest cluster centroid.\n",
    "Step 4: Recompute the centroids of newly formed clusters.\n",
    "Step 5: Repeat steps 3 and 4 until the centroids do not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6353249",
   "metadata": {},
   "source": [
    "#### 9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159a04e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Single Linkage: For two clusters R and S, the single linkage returns the minimum distance between two points i and j such that i belongs to R and j belongs to S.\n",
    "\n",
    "Complete Linkage: For two clusters R and S, the complete linkage returns the maximum distance between two points i and j such that i belongs to R and j belongs to S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca4b00",
   "metadata": {},
   "source": [
    "#### 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03422d4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Apriori algorithm assumes that any subset of a frequent itemset must be frequent. Its the algorithm behind Market Basket Analysis. So, according to the principle of Apriori, if {Grapes, Apple, Mango} is frequent, then {Grapes,Mango} must also be frequent. Now, using apriori concept we can make tables of all the items and their frequencies that occur together frequently. USing this table we can compute the confidence, support and lift metrics which aid in the business basket analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397c315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
